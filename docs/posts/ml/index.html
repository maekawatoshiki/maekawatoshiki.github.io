<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="この記事は何？ いきなりDeep Neural Networkの世界に足を踏み入れた人がいろいろ語るだけ あまり細かい話はしない 丸と矢印 深層学習だとかニューラルネットワーク（以下NN）だとか検索すると、以下のような画像をよく見かけると思う。全結合層があって、多分最後の方に活性化関数が挟んである。
入力にデータを流し込んであげると出力を得ることができる、とてもシンプルな構造をしている。
丸と矢印と… しかし、近年実際に使われているDNNはこれほど簡単な構造はしていない。
例えば、以下は文字認識を行うモデル(MNIST1)をONNX2形式としてエクスポートしてnetron3で表示したもの。
一目瞭然だが、先ほどの画像のNNとはだいぶ見た目が違う。丸くないし、いや、でも矢印はある。 丸の代わりに角丸の四角が現れた。矢印の横には謎の数字の列が書いてある。
ぱっと見、グラフ（DAG）であることはわかると思う。
軽く説明すると以下の通り。
角丸の四角: 命令（Operator; ONNX用語？真に受けないでほしい）。命令はデータに対してなんらかの操作を行う。 例えば、上の画像中の Relu はデータの一要素づつ（数値一つ一つ）に ReLU4 を適用する 一方、Conv はデータ全体に対して畳み込み5 を行う 命令ごとに、一要素づつ(element-wise)に操作を行なうのか、部分的に({layer,channel,row,&amp;hellip;}-wise)操作を行なうのか違ってややこしい 矢印: データの流れる方向。データというのは画像だったり文字列だったり… 謎の数字の列: テンソル6のshape。要するにデータがどのような形（のテンソル）なのかを表す。 なんと静的じゃないこともある。画像中ではすべての矢印の横に数字の列（＝shape)が書いてあるが、これは運がいいだけ。shape inference（=shapeを推論する）しても静的に shape が決まらないこともある。でもそのおかげで文字列とか複数のbounding boxを扱えたりする。 より複雑なモデル（画像を分類したりとか）は、もちろんより複雑な構造をしている。netron で表示すると重い。
グラフを計算機が理解するまで DNN はネットワークであり、グラフであり、そのままでは計算機上で動かない。
PyTorch, TensorFlow, Chainer, &amp;hellip; などの機械学習フレームワークは、設計思想の差こそあれ結局はNNモデルを計算機上で実際に動作させるためのものである。そこにはコンパイラと呼べるものが必ずある。いわゆる機械学習コンパイラである。
機械学習コンパイラのやることは、簡単に言えば以下の通りである。
NNモデルのグラフをより簡単な表現に変換する。命令（ノード）同士をくっつけたり分離したり、大胆に順番を入れ替えたり。（数学的に正しくなくてもいい！） 命令の実行順序を決定する。動的に変える場合もある。グラフ中には並列に実行できる命令が含まれるので気を付ける。 CPU, GPU, 独自アクセラレータなどごとに命令列を機械語へ変換。アセンブリを吐いたり、（サボって）LLVMとかOpenCLとかでカーネル吐いたり。CUDAで頑張る？ このあと、CPU, GPU, &amp;hellip; などが複雑に絡み合うヘテロジニアスな環境でどうやって高速にデータを受け渡すか、などの話もあるけどコンパイラの役割じゃない気がしたので省略。
現実 DNN は実際はただのグラフで、そこにデータを流し込んでるだけ。学習時はちょっと違うのかもしれないけど、少なくとも推論時はそう。
計算機をいかに高速に・効率的に動かすかを考えるのは面白い。けど、これが人工知能って呼ばれてるのをみるとなんか微妙な気持ちになるね。
（p.s. DNNの量子化の話聞きたい人いるかな？モデルのパラメータ数を減らしたり、モデル自体を小さくコンパクトにする系の話）
脚注 MNISTデータセット&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Open Neural Network Exchange の略。ここでは ONNX の定める、NNのモデルを表現するためのフォーマットを指す。&amp;#160;&amp;#x21a9;&amp;#xfe0e;" />
<meta name="keywords" content=", dnn" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://maekawatoshiki.github.io/posts/ml/" />









    <title>
        
            DNN雑記 :: Hello 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.de188b3201233c251f4fd6306dbd2cb41e408fb8846c09781b2925de7df5025c.css">






<meta itemprop="name" content="DNN雑記">
<meta itemprop="description" content="この記事は何？ いきなりDeep Neural Networkの世界に足を踏み入れた人がいろいろ語るだけ あまり細かい話はしない 丸と矢印 深層学習だとかニューラルネットワーク（以下NN）だとか検索すると、以下のような画像をよく見かけると思う。全結合層があって、多分最後の方に活性化関数が挟んである。
入力にデータを流し込んであげると出力を得ることができる、とてもシンプルな構造をしている。
丸と矢印と… しかし、近年実際に使われているDNNはこれほど簡単な構造はしていない。
例えば、以下は文字認識を行うモデル(MNIST1)をONNX2形式としてエクスポートしてnetron3で表示したもの。
一目瞭然だが、先ほどの画像のNNとはだいぶ見た目が違う。丸くないし、いや、でも矢印はある。 丸の代わりに角丸の四角が現れた。矢印の横には謎の数字の列が書いてある。
ぱっと見、グラフ（DAG）であることはわかると思う。
軽く説明すると以下の通り。
角丸の四角: 命令（Operator; ONNX用語？真に受けないでほしい）。命令はデータに対してなんらかの操作を行う。 例えば、上の画像中の Relu はデータの一要素づつ（数値一つ一つ）に ReLU4 を適用する 一方、Conv はデータ全体に対して畳み込み5 を行う 命令ごとに、一要素づつ(element-wise)に操作を行なうのか、部分的に({layer,channel,row,&hellip;}-wise)操作を行なうのか違ってややこしい 矢印: データの流れる方向。データというのは画像だったり文字列だったり… 謎の数字の列: テンソル6のshape。要するにデータがどのような形（のテンソル）なのかを表す。 なんと静的じゃないこともある。画像中ではすべての矢印の横に数字の列（＝shape)が書いてあるが、これは運がいいだけ。shape inference（=shapeを推論する）しても静的に shape が決まらないこともある。でもそのおかげで文字列とか複数のbounding boxを扱えたりする。 より複雑なモデル（画像を分類したりとか）は、もちろんより複雑な構造をしている。netron で表示すると重い。
グラフを計算機が理解するまで DNN はネットワークであり、グラフであり、そのままでは計算機上で動かない。
PyTorch, TensorFlow, Chainer, &hellip; などの機械学習フレームワークは、設計思想の差こそあれ結局はNNモデルを計算機上で実際に動作させるためのものである。そこにはコンパイラと呼べるものが必ずある。いわゆる機械学習コンパイラである。
機械学習コンパイラのやることは、簡単に言えば以下の通りである。
NNモデルのグラフをより簡単な表現に変換する。命令（ノード）同士をくっつけたり分離したり、大胆に順番を入れ替えたり。（数学的に正しくなくてもいい！） 命令の実行順序を決定する。動的に変える場合もある。グラフ中には並列に実行できる命令が含まれるので気を付ける。 CPU, GPU, 独自アクセラレータなどごとに命令列を機械語へ変換。アセンブリを吐いたり、（サボって）LLVMとかOpenCLとかでカーネル吐いたり。CUDAで頑張る？ このあと、CPU, GPU, &hellip; などが複雑に絡み合うヘテロジニアスな環境でどうやって高速にデータを受け渡すか、などの話もあるけどコンパイラの役割じゃない気がしたので省略。
現実 DNN は実際はただのグラフで、そこにデータを流し込んでるだけ。学習時はちょっと違うのかもしれないけど、少なくとも推論時はそう。
計算機をいかに高速に・効率的に動かすかを考えるのは面白い。けど、これが人工知能って呼ばれてるのをみるとなんか微妙な気持ちになるね。
（p.s. DNNの量子化の話聞きたい人いるかな？モデルのパラメータ数を減らしたり、モデル自体を小さくコンパクトにする系の話）
脚注 MNISTデータセット&#160;&#x21a9;&#xfe0e;
Open Neural Network Exchange の略。ここでは ONNX の定める、NNのモデルを表現するためのフォーマットを指す。&#160;&#x21a9;&#xfe0e;"><meta itemprop="datePublished" content="2021-12-23T19:47:09+09:00" />
<meta itemprop="dateModified" content="2021-12-23T19:47:09+09:00" />
<meta itemprop="wordCount" content="76"><meta itemprop="image" content="https://avatars.githubusercontent.com/u/9913176"/>
<meta itemprop="keywords" content="dnn," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://avatars.githubusercontent.com/u/9913176"/>

<meta name="twitter:title" content="DNN雑記"/>
<meta name="twitter:description" content="この記事は何？ いきなりDeep Neural Networkの世界に足を踏み入れた人がいろいろ語るだけ あまり細かい話はしない 丸と矢印 深層学習だとかニューラルネットワーク（以下NN）だとか検索すると、以下のような画像をよく見かけると思う。全結合層があって、多分最後の方に活性化関数が挟んである。
入力にデータを流し込んであげると出力を得ることができる、とてもシンプルな構造をしている。
丸と矢印と… しかし、近年実際に使われているDNNはこれほど簡単な構造はしていない。
例えば、以下は文字認識を行うモデル(MNIST1)をONNX2形式としてエクスポートしてnetron3で表示したもの。
一目瞭然だが、先ほどの画像のNNとはだいぶ見た目が違う。丸くないし、いや、でも矢印はある。 丸の代わりに角丸の四角が現れた。矢印の横には謎の数字の列が書いてある。
ぱっと見、グラフ（DAG）であることはわかると思う。
軽く説明すると以下の通り。
角丸の四角: 命令（Operator; ONNX用語？真に受けないでほしい）。命令はデータに対してなんらかの操作を行う。 例えば、上の画像中の Relu はデータの一要素づつ（数値一つ一つ）に ReLU4 を適用する 一方、Conv はデータ全体に対して畳み込み5 を行う 命令ごとに、一要素づつ(element-wise)に操作を行なうのか、部分的に({layer,channel,row,&hellip;}-wise)操作を行なうのか違ってややこしい 矢印: データの流れる方向。データというのは画像だったり文字列だったり… 謎の数字の列: テンソル6のshape。要するにデータがどのような形（のテンソル）なのかを表す。 なんと静的じゃないこともある。画像中ではすべての矢印の横に数字の列（＝shape)が書いてあるが、これは運がいいだけ。shape inference（=shapeを推論する）しても静的に shape が決まらないこともある。でもそのおかげで文字列とか複数のbounding boxを扱えたりする。 より複雑なモデル（画像を分類したりとか）は、もちろんより複雑な構造をしている。netron で表示すると重い。
グラフを計算機が理解するまで DNN はネットワークであり、グラフであり、そのままでは計算機上で動かない。
PyTorch, TensorFlow, Chainer, &hellip; などの機械学習フレームワークは、設計思想の差こそあれ結局はNNモデルを計算機上で実際に動作させるためのものである。そこにはコンパイラと呼べるものが必ずある。いわゆる機械学習コンパイラである。
機械学習コンパイラのやることは、簡単に言えば以下の通りである。
NNモデルのグラフをより簡単な表現に変換する。命令（ノード）同士をくっつけたり分離したり、大胆に順番を入れ替えたり。（数学的に正しくなくてもいい！） 命令の実行順序を決定する。動的に変える場合もある。グラフ中には並列に実行できる命令が含まれるので気を付ける。 CPU, GPU, 独自アクセラレータなどごとに命令列を機械語へ変換。アセンブリを吐いたり、（サボって）LLVMとかOpenCLとかでカーネル吐いたり。CUDAで頑張る？ このあと、CPU, GPU, &hellip; などが複雑に絡み合うヘテロジニアスな環境でどうやって高速にデータを受け渡すか、などの話もあるけどコンパイラの役割じゃない気がしたので省略。
現実 DNN は実際はただのグラフで、そこにデータを流し込んでるだけ。学習時はちょっと違うのかもしれないけど、少なくとも推論時はそう。
計算機をいかに高速に・効率的に動かすかを考えるのは面白い。けど、これが人工知能って呼ばれてるのをみるとなんか微妙な気持ちになるね。
（p.s. DNNの量子化の話聞きたい人いるかな？モデルのパラメータ数を減らしたり、モデル自体を小さくコンパクトにする系の話）
脚注 MNISTデータセット&#160;&#x21a9;&#xfe0e;
Open Neural Network Exchange の略。ここでは ONNX の定める、NNのモデルを表現するためのフォーマットを指す。&#160;&#x21a9;&#xfe0e;"/>



    <meta property="og:title" content="DNN雑記" />
<meta property="og:description" content="この記事は何？ いきなりDeep Neural Networkの世界に足を踏み入れた人がいろいろ語るだけ あまり細かい話はしない 丸と矢印 深層学習だとかニューラルネットワーク（以下NN）だとか検索すると、以下のような画像をよく見かけると思う。全結合層があって、多分最後の方に活性化関数が挟んである。
入力にデータを流し込んであげると出力を得ることができる、とてもシンプルな構造をしている。
丸と矢印と… しかし、近年実際に使われているDNNはこれほど簡単な構造はしていない。
例えば、以下は文字認識を行うモデル(MNIST1)をONNX2形式としてエクスポートしてnetron3で表示したもの。
一目瞭然だが、先ほどの画像のNNとはだいぶ見た目が違う。丸くないし、いや、でも矢印はある。 丸の代わりに角丸の四角が現れた。矢印の横には謎の数字の列が書いてある。
ぱっと見、グラフ（DAG）であることはわかると思う。
軽く説明すると以下の通り。
角丸の四角: 命令（Operator; ONNX用語？真に受けないでほしい）。命令はデータに対してなんらかの操作を行う。 例えば、上の画像中の Relu はデータの一要素づつ（数値一つ一つ）に ReLU4 を適用する 一方、Conv はデータ全体に対して畳み込み5 を行う 命令ごとに、一要素づつ(element-wise)に操作を行なうのか、部分的に({layer,channel,row,&hellip;}-wise)操作を行なうのか違ってややこしい 矢印: データの流れる方向。データというのは画像だったり文字列だったり… 謎の数字の列: テンソル6のshape。要するにデータがどのような形（のテンソル）なのかを表す。 なんと静的じゃないこともある。画像中ではすべての矢印の横に数字の列（＝shape)が書いてあるが、これは運がいいだけ。shape inference（=shapeを推論する）しても静的に shape が決まらないこともある。でもそのおかげで文字列とか複数のbounding boxを扱えたりする。 より複雑なモデル（画像を分類したりとか）は、もちろんより複雑な構造をしている。netron で表示すると重い。
グラフを計算機が理解するまで DNN はネットワークであり、グラフであり、そのままでは計算機上で動かない。
PyTorch, TensorFlow, Chainer, &hellip; などの機械学習フレームワークは、設計思想の差こそあれ結局はNNモデルを計算機上で実際に動作させるためのものである。そこにはコンパイラと呼べるものが必ずある。いわゆる機械学習コンパイラである。
機械学習コンパイラのやることは、簡単に言えば以下の通りである。
NNモデルのグラフをより簡単な表現に変換する。命令（ノード）同士をくっつけたり分離したり、大胆に順番を入れ替えたり。（数学的に正しくなくてもいい！） 命令の実行順序を決定する。動的に変える場合もある。グラフ中には並列に実行できる命令が含まれるので気を付ける。 CPU, GPU, 独自アクセラレータなどごとに命令列を機械語へ変換。アセンブリを吐いたり、（サボって）LLVMとかOpenCLとかでカーネル吐いたり。CUDAで頑張る？ このあと、CPU, GPU, &hellip; などが複雑に絡み合うヘテロジニアスな環境でどうやって高速にデータを受け渡すか、などの話もあるけどコンパイラの役割じゃない気がしたので省略。
現実 DNN は実際はただのグラフで、そこにデータを流し込んでるだけ。学習時はちょっと違うのかもしれないけど、少なくとも推論時はそう。
計算機をいかに高速に・効率的に動かすかを考えるのは面白い。けど、これが人工知能って呼ばれてるのをみるとなんか微妙な気持ちになるね。
（p.s. DNNの量子化の話聞きたい人いるかな？モデルのパラメータ数を減らしたり、モデル自体を小さくコンパクトにする系の話）
脚注 MNISTデータセット&#160;&#x21a9;&#xfe0e;
Open Neural Network Exchange の略。ここでは ONNX の定める、NNのモデルを表現するためのフォーマットを指す。&#160;&#x21a9;&#xfe0e;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://maekawatoshiki.github.io/posts/ml/" /><meta property="og:image" content="https://avatars.githubusercontent.com/u/9913176"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-23T19:47:09+09:00" />
<meta property="article:modified_time" content="2021-12-23T19:47:09+09:00" /><meta property="og:site_name" content="Hello" />







    <meta property="article:published_time" content="2021-12-23 19:47:09 &#43;0900 JST" />







<script async src="https://www.googletagmanager.com/gtag/js?id=UA-59523618-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59523618-3');
</script>


    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">uint256_t&#39;s blog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">About</a></li><li><a href="/posts">Blog</a></li><li><a href="/qr">QR</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        One minute

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://maekawatoshiki.github.io/posts/ml/">DNN雑記</a>
      </h1>

      

      

      <div class="post-content">
        <h1 id="この記事は何">この記事は何？</h1>
<ul>
<li>いきなりDeep Neural Networkの世界に足を踏み入れた人がいろいろ語るだけ</li>
<li>あまり細かい話はしない</li>
</ul>
<h1 id="丸と矢印">丸と矢印</h1>
<p>深層学習だとかニューラルネットワーク（以下NN）だとか検索すると、以下のような画像をよく見かけると思う。全結合層があって、多分最後の方に活性化関数が挟んである。</p>
<p><img src="https://miro.medium.com/max/4800/1*3fA77_mLNiJTSgZFhYnU0Q.png" alt="nn"></p>
<p>入力にデータを流し込んであげると出力を得ることができる、とてもシンプルな構造をしている。</p>
<h1 id="丸と矢印と">丸と矢印と…</h1>
<p>しかし、近年実際に使われているDNNはこれほど簡単な構造はしていない。</p>
<p>例えば、以下は文字認識を行うモデル(MNIST<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>)をONNX<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>形式としてエクスポートして<code>netron</code><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>で表示したもの。</p>
<p><img src="https://cdn.thenewstack.io/media/2020/07/c601845f-onnx-mnist-0-328x1024.jpg" alt="mnist"></p>
<p>一目瞭然だが、先ほどの画像のNNとはだいぶ見た目が違う。丸くないし、いや、でも矢印はある。
丸の代わりに角丸の四角が現れた。矢印の横には謎の数字の列が書いてある。</p>
<p>ぱっと見、グラフ（DAG）であることはわかると思う。</p>
<p>軽く説明すると以下の通り。</p>
<ul>
<li>角丸の四角: <strong>命令</strong>（Operator; ONNX用語？真に受けないでほしい）。命令はデータに対してなんらかの操作を行う。
<ul>
<li>例えば、上の画像中の <code>Relu</code> はデータの一要素づつ（数値一つ一つ）に ReLU<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> を適用する</li>
<li>一方、<code>Conv</code> はデータ全体に対して畳み込み<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> を行う
<ul>
<li>命令ごとに、一要素づつ(element-wise)に操作を行なうのか、部分的に({layer,channel,row,&hellip;}-wise)操作を行なうのか違ってややこしい</li>
</ul>
</li>
</ul>
</li>
<li>矢印: データの流れる方向。データというのは画像だったり文字列だったり…</li>
<li>謎の数字の列: テンソル<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>のshape。要するにデータがどのような形（のテンソル）なのかを表す。
<ul>
<li>なんと静的じゃないこともある。画像中ではすべての矢印の横に数字の列（＝shape)が書いてあるが、これは運がいいだけ。shape inference（=shapeを推論する）しても静的に shape が決まらないこともある。でもそのおかげで文字列とか複数のbounding boxを扱えたりする。</li>
</ul>
</li>
</ul>
<p>より複雑なモデル（画像を分類したりとか）は、もちろんより複雑な構造をしている。netron で表示すると重い。</p>
<h1 id="グラフを計算機が理解するまで">グラフを計算機が理解するまで</h1>
<p>DNN はネットワークであり、グラフであり、そのままでは計算機上で動かない。</p>
<p>PyTorch, TensorFlow, Chainer, &hellip; などの機械学習フレームワークは、設計思想の差こそあれ結局はNNモデルを計算機上で実際に動作させるためのものである。そこにはコンパイラと呼べるものが必ずある。いわゆる機械学習コンパイラである。</p>
<p>機械学習コンパイラのやることは、簡単に言えば以下の通りである。</p>
<ol>
<li>NNモデルのグラフをより簡単な表現に変換する。命令（ノード）同士をくっつけたり分離したり、大胆に順番を入れ替えたり。（数学的に正しくなくてもいい！）</li>
<li>命令の実行順序を決定する。動的に変える場合もある。グラフ中には並列に実行できる命令が含まれるので気を付ける。</li>
<li>CPU, GPU, 独自アクセラレータなどごとに命令列を機械語へ変換。アセンブリを吐いたり、（サボって）LLVMとかOpenCLとかでカーネル吐いたり。CUDAで頑張る？</li>
</ol>
<p>このあと、CPU, GPU, &hellip; などが複雑に絡み合うヘテロジニアスな環境でどうやって高速にデータを受け渡すか、などの話もあるけどコンパイラの役割じゃない気がしたので省略。</p>
<h1 id="現実">現実</h1>
<p>DNN は実際はただのグラフで、そこにデータを流し込んでるだけ。学習時はちょっと違うのかもしれないけど、少なくとも推論時はそう。</p>
<p>計算機をいかに高速に・効率的に動かすかを考えるのは面白い。けど、これが人工知能って呼ばれてるのをみるとなんか微妙な気持ちになるね。</p>
<p>（p.s. DNNの量子化の話聞きたい人いるかな？モデルのパラメータ数を減らしたり、モデル自体を小さくコンパクトにする系の話）</p>
<h1 id="脚注">脚注</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://ja.wikipedia.org/wiki/MNIST%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9">MNISTデータセット</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://onnx.ai/">Open Neural Network Exchange</a> の略。ここでは ONNX の定める、NNのモデルを表現するためのフォーマットを指す。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://github.com/lutzroeder/netron">netron</a>。NNのモデルを可視化してくれる。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://ja.wikipedia.org/wiki/%E6%AD%A3%E8%A6%8F%E5%8C%96%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0">ReLU</a>。角が尖ってる。&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://ja.wikipedia.org/wiki/%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">Convolution</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>行列は2階のテンソル。要するに高次元の行列だと思えばわかりやすい。 <a href="https://qiita.com/enoughspacefor/items/66982f595e2c71502062#%E8%B3%87%E6%96%993tensor">これがわかりやすい？</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://maekawatoshiki.github.io/tags/dnn/">dnn</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        76 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2021-12-23 19:47
        

         
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://maekawatoshiki.github.io/posts/seccamp2022/">
                <span class="button__icon">←</span>
                <span class="button__text">セキュリティ・キャンプ 2022 参加記（講師視点）</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://maekawatoshiki.github.io/posts/gsoc3/">
                <span class="button__text">Google Summer of Code 2021 を終えました</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    

    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025</span>
            
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span><span><a href="https://maekawatoshiki.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
            <span>Made with &#10084; by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
          </div>
    </div>
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.8d7489314e5e17deefb28c0dc01699f35b89fed0c3748ade7d90114f12f4b4bf4faa837c3ce0d8c0c933ca89b210b48f201bff13782c74a5b21ddf9f5df61edd.js" integrity="sha512-jXSJMU5eF97vsowNwBaZ81uJ/tDDdIrefZARTxL0tL9PqoN8PODYwMkzyomyELSPIBv/E3gsdKWyHd&#43;fXfYe3Q=="></script>



    </body>
</html>
